<!-- Template from Jon Barron -->
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Yue Yang 杨樾</title>
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-111341597-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-111341597-1');
    </script>
  </head>
  <body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="60%" valign="middle">
              <name>Yue Yang (杨樾)</name><br>
              pronounced as <i>yoo-eh</i><br><br>
              Moore 103 (SIG Lab), 3300 Walnut St<br>Philadelphia, PA 19104, USA<br>
              Email: yueyang1 [at] seas.upenn.edu<br><br>
              <a href="https://scholar.google.com/citations?user=uvyYzagAAAAJ&hl=en&authuser=1">Google Scholar</a> &nbsp; / &nbsp; <a href="https://github.com/YueYANG1996">GitHub</a> &nbsp; / &nbsp; <a href="linkedin.com/in/yue-yang-4b2075174">LinkedIn</a> &nbsp; / &nbsp; <a href="https://www.youtube.com/channel/UCRV7M6KW5BZptw_aP0A-JuA">Youtube</a> &nbsp; / &nbsp; <a href="https://www.pixiv.net/users/14283763">pixiv</a> &nbsp; / &nbsp; <a href="cv.pdf">CV</a>
            </td>
            <td width="40%">
              <img src="images/portrait.jpg" width="100%">
            </td>
          </tr>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <!-- About me -->
          <tr>
            <td width="100%" valign="middle">
              <heading>About me</heading><br><br>
                <p class="light">Hi! My name is Yue Yang (杨樾). I am a second-year Ph.D. student in Computer and Information Science at the University of Pennsylvania, affiliated with <a href="https://nlp.cis.upenn.edu">Penn NLP</a>. I am grateful to be advised by <a href="https://www.cis.upenn.edu/~ccb/">Prof. Chris Callison-Burch</a> and <a href="http://markyatskar.com">Prof. Mark Yatskar</a>. 
                </p>
                <p> I am interested in the intersection area of Natural Language Processing (NLP) and Computer Vision (CV), aka <a href="https://en.wikipedia.org/wiki/Multimodality">Multimodal</a>. My current research focuses on schema induction via multimodal information.
                </p>
                <p>
                  Before Penn, I was an undergrad in Mechanical Engineering at the <a href="http://www.doe.zju.edu.cn/doeen/">College of Energy Engineering</a> at Zhejiang University. I worked with <a href="https://person.zju.edu.cn/en/0010729"> Prof. Yuqi Huang</a> to study Computational Fluid Dynamics (CFD) during undergrads.
                </p>
                <p>
                  Penn initially admitted me as a master's student in <a href="https://www.grasp.upenn.edu/academics/masters-degree-program/"> Robotics</a> in 2018. Now I switch to Artificial Intelligence and temporarily farewell to the hardware (Not wholly True, I designed a sensor pack for R2D2, which students of <a href="http://artificial-intelligence-class.org/r2d2_assignments/20fall/hw0/sensor-pack-setup.html"> CIS-521</a>) will use).
                </p>
            </td>
          </tr>
          </table>

          <!-- Publications & Manuscripts -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="40%" valign="middle">
                <heading>Publications & Manuscripts</heading>
              </td>
            </tr>
            <tr>
              <td width="30%">
                <img src="images/IER.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/IER.pdf"><b>Induce, Edit, Retrieve: Language Grounded Multimodal Schema for Instructional Video Retrieval</b></a>
                </div>
                <div class="authors">
                    <strong>Yue Yang</strong>, Joongwon Kim, Artemis Panagopoulou, Mark Yatskar and Chris Callison-Burch
                </div>
                <div class="venue">
                  <i>Preprint</i>, 2021
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2111.09276">arxiv</a> /
                  <a href="bib/yang2021induce.bib">bibtex</a>
                </div>
                <div>
                  TL;DR: This work proposes a novel system that induces schemata from web videos and generalizes them to capture unseen tasks with the goal of improving video retrieval performance, and demonstrates that the schemata induced by the system are better than those generated by other models.
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/VGSI.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/VGSI.pdf"><b>Visual Goal-Step Inference using wikiHow</b></a>
                </div>
                <div class="authors">
                    <strong>Yue Yang</strong>, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar and Chris Callison-Burch
                </div>
                <div class="venue">
                  <i>Conference on Empirical Methods in Natural Language Processing (<b>EMNLP</b>)</i>, 2021
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2104.05845">arxiv</a> / 
                  <a href="bib/yang2021visual.bib">bibtex</a> /
                  <a href="https://www.youtube.com/watch?v=V3Y_56ykG54&t=11s">talk</a> /
                  <a href="https://github.com/YueYANG1996/wikiHow-VGSI">github</a>
                </div>
                <div>
                  TL;DR: This work proposes the Visual Goal-Step Inference (VGSI) task where a model is given a textual goal and must choose a plausible step towards that goal from among four candidate images.
                </div>
              </td>
            </tr>


          </table>

          <!-- Acknowledgements -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
              <td><br>
              <p align="right">
              <font size="2">
              Website source from <a href="https://jonbarron.info">Jon Barron</a>.
          </tr>
          </table>
        </td>
      </tr>
    </table>
  </body>
<script>'undefined'=== typeof _trfq || (window._trfq = []);'undefined'=== typeof _trfd && (window._trfd=[]),_trfd.push({'tccl.baseHost':'secureserver.net'}),_trfd.push({'ap':'cpsh-oh'},{'server':'p3plzcpnl472835'},{'id':'7914943'}) // Monitoring performance to make your website faster. If you want to opt-out, please contact web hosting support.</script><script src='https://img1.wsimg.com/tcc/tcc_l.combined.1.0.6.min.js'></script></html>
