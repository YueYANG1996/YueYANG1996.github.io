<!-- Template from Jon Barron -->
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Yue Yang 杨樾</title>
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <link rel="icon" href="images/icons/HoloDeckLogo.svg">
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-111341597-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-111341597-1');
    </script>

    <style>
      .container {
          display: flex;
          align-items: center;
      }
      .container img {
          height: 40px; /* Adjust as needed */
          margin-right: 10px; /* Space between text and image */
      }
      .text {
          line-height: 20px; /* Adjust based on text size */
      }
    </style>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro|Crimson+Text|Noto+Sans+SC"
    rel="stylesheet">

    <script>
      function toggleLists() {
        var shortList = document.getElementById("shortList");
        var fullList = document.getElementById("fullList");

        if (shortList.style.display === "none") {
            shortList.style.display = "block";
            fullList.style.display = "none";
        } else {
            shortList.style.display = "none";
            fullList.style.display = "block";
        }
      }
      function showFullList() {
          document.getElementById("shortList").style.display = "none";
          document.getElementById("fullList").style.display = "block";
      }

      function showShortList() {
          document.getElementById("fullList").style.display = "none";
          document.getElementById("shortList").style.display = "block";
      }
    </script>
  </head>
  <body>
    <table width="930" border="0" align="center" cellspacing="0" cellpadding="0">
      <tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="70%" valign="middle">
              <name>Yue Yang</name><br>
              pronounced as <i>yoo-eh</i><br><br>
              220 South 33rd Street, Towne 299<br>Philadelphia, PA 19104, USA<br>
              Email: yueyang1 [at] seas.upenn.edu<br><br>
              <a href="https://scholar.google.com/citations?user=uvyYzagAAAAJ&hl=en&authuser=1">
                <img src="images/icons/google-scholar.svg" alt="Google Scholar Icon">
              </a>
              <a href="https://www.semanticscholar.org/author/Yue-Yang/2109409802">
                <img src="images/icons/semantic-scholar.svg" alt="Semantic Scholar Icon">
              </a>
              <a href="https://github.com/YueYANG1996">
                <img src="images/icons/github.svg" alt="GitHub Icon">
              </a>
              <a href="https://twitter.com/YueYangAI">
                <img src="images/icons/twitter.svg" alt="Twitter Icon">
              </a>
              <a href="https://www.linkedin.com/in/yue-yang-4b2075174">
                <img src="images/icons/linkedin.svg" alt="LinkedIn Icon">
              </a>
              <a href="https://www.youtube.com/channel/UCRV7M6KW5BZptw_aP0A-JuA">
                <img src="images/icons/youtube.svg" alt="Youtube Icon">
              </a>
              <a href="https://www.pixiv.net/users/14283763">
                <img src="images/icons/pixiv.svg" alt="Pixiv Icon">
              </a>
              <a href="cv.pdf">
                <img src="images/icons/curriculum-vitae.svg" alt="CV">
              </a>
            </td>

            <td width="30%">
              <img src="images/cancun.png" width="100%">
            </td>
          </tr>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <!-- About me -->
          <tr>
            <td width="100%" valign="middle">
              <heading>About me</heading>
                <div class="text" align="justify">
                  <p class="light">Hi! My name is Yue Yang (杨樾). I am a final-year Ph.D. student in Computer and Information Science at the University of Pennsylvania, affiliated with <a href="https://nlp.cis.upenn.edu">Penn NLP</a>. I am grateful to be advised by <a href="https://www.cis.upenn.edu/~ccb/">Prof. Chris Callison-Burch</a> and <a href="https://www.cis.upenn.edu/~myatskar/">Prof. Mark Yatskar</a>. I am interested in the intersection area of Natural Language Processing (NLP) and Computer Vision (CV).
                  </p>
                </div>
                <div class="text" align="justify">
                  <p>
                    My current research focuses on applying the knowledge priors of large language models (LLMs) to various domains (images, videos, healthcare, Embodied AI, etc) to improve different aspects of AI systems, including:
                  </p>
                </div>
                <div class="container">
                  <img src="images/interpretability.png">
                  <div class="text" align="justify">
                      <span><strong>Interpretability.</strong> LLMs aid in constructing human-readable intermediate representations, such as concept bottlenecks, enabling the design of inherently interpretable models, thereby mitigating the black-box nature of deep learning.</span><br>
                  </div>
                </div>
                <br>
                <div class="container">
                  <img src="images/robustness.png">
                  <div class="text" align="justify">
                      <span><strong>Robustness.</strong> By utilizing sparse natural language representations as input, models are less prone to overfitting on the spurious cues of in-domain training data, enhancing their robustness and out-of-domain generalization.</span><br>
                  </div>
                </div>
                <br>
                <div class="container">
                  <img src="images/creativity.png">
                  <div class="text" align="justify">
                      <span><strong>Controllability & Creativity.</strong> Language interfaces in generative systems enables easier control over the generation process. Leveraging the extensive world knowledge of LLMs, these systems can produce customized and diverse outputs. </span><br>
                  </div>
                </div>
                <br>
                <p>
                  <b style="color: #ac530f">I am looking for full-time positions starting in Summer 2025. Please reach out if you are interested in working with me!</b>
                </p>
                <!-- <img src="images/research_direction.png" width="80%"> -->
                <!-- <p>
                  Before Penn, I was an undergrad in Mechanical Engineering at the <a href="http://www.doe.zju.edu.cn/doeen/">College of Energy Engineering</a> at Zhejiang University. I worked with <a href="https://person.zju.edu.cn/en/0010729"> Prof. Yuqi Huang</a> to study Computational Fluid Dynamics (CFD) during undergrads.
                </p>
                <p>
                  Penn initially admitted me as a master's student in <a href="https://www.grasp.upenn.edu/academics/masters-degree-program/"> Robotics</a> in 2018. Now I switch to Artificial Intelligence and temporarily farewell to the hardware (Not wholly True. I designed a sensor pack for R2D2, which students of <a href="https://artificial-intelligence-class.org/r2d2_assignments/22fall/hw0/hw0.html"> CIS-521</a> will use).
                </p> -->
            </td>
          </tr>
          </table>

          <!-- Publications & Manuscripts -->

          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="30%" valign="middle">
                <heading>Recent Preprints</heading>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/retina-teaser.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/retina.pdf"><b>A Concept-based Interpretable Model for the Diagnosis of Choroid Neoplasias using Multimodal Data</b></a>
                </div>
                <div class="authors">
                  Yifan Wu, Yang Liu, <strong>Yue Yang</strong>, Michael S. Yao, Wenli Yang, Xuehui Shi, Lihong Yang, Dongjun Li, Yueming Liu, James C. Gee, Xuan Yang, Wen-bin Wei, Shi Gu
                </div>
                <div class="venue">
                    arxiv, 2024
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2403.05606">arxiv</a> /
                  <a href="https://mmcbm.liuy.site/">demo</a>
                </div>
                <div align="justify">
                  <!-- TL;DR: Text Bottleneck Model (TBM) is an inheretly interpretable text classification framework that offers both global and local explanations by iteratively constructing concept bottlenecks. TBM can rival the performance of established black-box baselines such as GPT-4 fewshot and finetuned DeBERTa. -->
                <!-- </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/TBM.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/TBM.pdf"><b>Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck</b></a>
                </div>
                <div class="authors">
                  Josh Magnus Ludan, Qing Lyu, <strong>Yue Yang</strong>, Liam Dugan, Mark Yatskar, Chris Callison-Burch
                </div>
                <div class="venue">
                    arxiv, 2023
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2310.19660">arxiv</a> /
                  <a href="https://github.com/JMRLudan/TBM">code</a>
                </div>
                <div align="justify">
                  TL;DR: Text Bottleneck Model (TBM) is an inheretly interpretable text classification framework that offers both global and local explanations by iteratively constructing concept bottlenecks. TBM can rival the performance of established black-box baselines such as GPT-4 fewshot and finetuned DeBERTa.
                </div>
              </td>
            </tr>
          </table> -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" id="shortList">
            <!-- Your selected publications rows here -->
            <tr>
              <td width="30%" valign="middle">
                <heading>Selected Works</heading>
              </td>
              <td width="70%" valign="right">
                <div align="right">
                  <a href="javascript:void(0);" onclick="showFullList()">Full List of Publications</a>
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/molmo.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="https://molmo.allenai.org/blog"><b>Open Weights and Open Data for State-of-the-Art Multimodal Models</b></a>
                </div>
                <div class="authors">
                  Matt Deitke*, Christopher Clark*, Sangho Lee, Rohun Tripathi, <strong>Yue Yang</strong>, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, et al. (51 authors in total)
                </div>
                <div class="tags">
                  <a href="https://molmo.allenai.org/blog">blog</a> /
                  <a href="https://molmo.allenai.org">demo</a> /
                  <a href="https://molmo.allenai.org/paper.pdf">report</a> /
                  <a href="https://huggingface.co/collections/allenai/molmo-66f379e6fe3b8ef090a8ca19">models</a>
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/knobo-teaser.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/knobo.pdf"><b>A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis</b></a>
                </div>
                <div class="authors">
                  <strong>Yue Yang</strong>, Mona Gandhi, Yufei Wang, Yifan Wu, Michael S. Yao, Chris Callison-Burch, James C. Gee, Mark Yatskar
                </div>
                <div class="venue">
                  <i>Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</i>, 2024 <b style="color: #ac530f">(Spotlight)</b>
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2405.14839">arxiv</a> /
                  <a href="https://yueyang1996.github.io/knobo/">website</a> /
                  <a href="https://github.com/YueYANG1996/KnoBo">code</a>
                </div>
                <div align="justify">
                  TL;DR: We introduce Knowledge Bottlenecks (KnoBo) that incorporate priors from medical documents, such as PubMed, through inherently interpretable models. KnoBo is robust to domain shifts in medical images, e.g., data sampled from different hospitals or data confounded by demographic variables such as sex, race, etc. Overall, our work demonstrates that a key missing ingredient for robustness to distribution shift in medical imaging models is a prior rooted in knowledge.
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <div class="caption" style="font-size: 12px;">
                  <center>
                    a 1b1b apartment of a researcher who has a cat
                  </center>  
                </div>
                <video poster="" autoplay="" muted loop="" style="pointer-events: none; width: 220px;">
                  <source src="images/a_1b1b_apartment_of_a_researcher_who_has_a_cat_0_0_sunny_vondelpark_4k_1.0_100.0_360_16.0_30_10_CYCLES_video.mp4" type="video/mp4">
                </video>
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/holodeck.pdf"><b><span class="small-caps">Holodeck</span>: Language Guided Generation of 3D Embodied AI Environments</b></a>
                </div>
                <div class="authors">
                    <strong>Yue Yang</strong>*, Fan-Yun Sun*, Luca Weihs*, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, Christopher Clark
                </div>
                <div class="venue">
                    <i>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</i>, 2024
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2312.09067">arxiv</a> / <a href="https://yueyang1996.github.io/holodeck/">website</a> / <a href="https://github.com/allenai/Holodeck">code</a> / <a href="https://ai.seas.upenn.edu/news/penn-engineers-recreate-star-treks-holodeck-using-chatgpt-and-video-game-assets/">press</a>
                </div>
                <div align="justify">
                  TL;DR: <span class="small-caps">Holodeck</span> is an automated system for generating diverse 3D environments in Embodied AI, using a large language model (GPT-4) and a vast collection of 3D assets from Objaverse. It can create complex scenes based on user prompts, adjusting for styles and specific details, like "a 1b1b apartment of a researcher who has a cat".
                </div>
              </td>
            </tr>
            
            <tr>
              <td width="30%">
                <img src="images/labo.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/labo.pdf"><b>Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification</b></a>
                </div>
                <div class="authors">
                    <strong>Yue Yang</strong>, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, Mark Yatskar
                </div>
                <div class="venue">
                   <i>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</i>, 2023
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2211.11158">arxiv</a> /
                  <a href="https://github.com/YueYANG1996/LaBo">code</a>
                </div>
                <div align="justify">
                  TL;DR: Concept Bottleneck Models are interpretable models that factor in human-readable concepts to explain model decisions. However, CBMs often under-perform their black box counterparts and require manual specification of concepts. Our method, LaBo, leverages large language models (GPT-3) to automatically construct bottlenecks for any image classification tasks.
                </div>
              </td>
            </tr>
          </table>          

          <!-- Full List (initially hidden) -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" id="fullList" style="display: none;">
            <!-- Your full list of publications rows here -->
            <tr>
              <td width="30%" valign="middle">
                <heading>Publications</heading> <br>
              </td>

              <td width="70%" valign="right">
                <div align="right">
                  <a href="javascript:void(0);" onclick="showShortList()">Selected Publications</a>
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/knobo-teaser.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/knobo.pdf"><b>A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis</b></a>
                </div>
                <div class="authors">
                  <strong>Yue Yang</strong>, Mona Gandhi, Yufei Wang, Yifan Wu, Michael S. Yao, Chris Callison-Burch, James C. Gee, Mark Yatskar
                </div>
                <div class="venue">
                  <i>Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</i>, 2024 <b style="color: #ac530f">(Spotlight)</b>
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2405.14839">arxiv</a> /
                  <a href="https://yueyang1996.github.io/knobo/">website</a> /
                  <a href="https://github.com/YueYANG1996/KnoBo">code</a>
                </div>
                <div align="justify">
                  TL;DR: We introduce Knowledge Bottlenecks (KnoBo) that incorporate priors from medical documents, such as PubMed, through inherently interpretable models. KnoBo is robust to domain shifts in medical images, e.g., data sampled from different hospitals or data confounded by demographic variables such as sex, race, etc. Overall, our work demonstrates that a key missing ingredient for robustness to distribution shift in medical imaging models is a prior rooted in knowledge.
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/news.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/knobo.pdf"><b>MiRAGeNews: Multimodal Realistic AI-Generated News Detection</b></a>
                </div>
                <div class="authors">
                  Runsheng Huang, Liam Dugan, Yue Yang, Chris Callison-Burch.
                </div>
                <div class="venue">
                  <i>Findings of the Empirical Methods in Natural Language Processing (<b>EMNLP</b>)</i>, 2024
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2405.14839">arxiv</a> /
                  <a href="https://yueyang1996.github.io/knobo/">website</a> /
                  <a href="https://github.com/YueYANG1996/KnoBo">code</a>
                </div>
                <div align="justify">
                  TL;DR: We propose a multimodal fake news dataset for detecting AI-generated content.
              </td>
            </tr>

            <tr>
              <td width="30%">
                <video poster="" autoplay="" muted loop="" style="pointer-events: none; width: 220px;">
                  <source src="images/como_demo.mp4" type="video/mp4">
                </video>
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/como.pdf"><b>CoMo: Controllable Motion Generation through Language Guided Pose Code Editing</b></a>
                </div>
                <div class="authors">
                  Yiming Huang, Weilin Wan, <strong>Yue Yang</strong>, Chris Callison-Burch, Mark Yatskar, Lingjie Liu
                </div>
                <div class="venue">
                  <i>European Conference on Computer Vision (<b>ECCV</b>)</i>, 2024
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2403.13900">arxiv</a> /
                  <a href="https://yh2371.github.io/como/">website</a>
                </div>
                <div align="justify">
                  TL;DR: CoMo is a controllable human motion generation model that encodes motion into interpretable pose codes representing body part semantics. Leveraging pose codes as interpretable representations, an LLM can directly intervene in motion editing by adjusting the pose codes according to editing instructions.
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <div class="caption" style="font-size: 12px;">
                  <center>
                    a 1b1b apartment of a researcher who has a cat
                  </center>  
                </div>
                <video poster="" autoplay="" muted loop="" style="pointer-events: none; width: 220px;">
                  <source src="images/a_1b1b_apartment_of_a_researcher_who_has_a_cat_0_0_sunny_vondelpark_4k_1.0_100.0_360_16.0_30_10_CYCLES_video.mp4" type="video/mp4">
                </video>
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/holodeck.pdf"><b><span class="small-caps">Holodeck</span>: Language Guided Generation of 3D Embodied AI Environments</b></a>
                </div>
                <div class="authors">
                    <strong>Yue Yang</strong>*, Fan-Yun Sun*, Luca Weihs*, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, Christopher Clark
                </div>
                <div class="venue">
                    <i>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</i>, 2024
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2312.09067">arxiv</a> / <a href="https://yueyang1996.github.io/holodeck/">website</a> / <a href="https://github.com/allenai/Holodeck">code</a>
                </div>
                <div align="justify">
                  TL;DR: <span class="small-caps">Holodeck</span> is an automated system for generating diverse 3D environments in Embodied AI, using a large language model (GPT-4) and a vast collection of 3D assets from Objaverse. It can create complex scenes based on user prompts, adjusting for styles and specific details, like "a 1b1b apartment of a researcher who has a cat".
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%" style="text-align:center;">
                <img src="images/metaphor.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="https://arxiv.org/pdf/2305.14724.pdf"><b>I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors</b></a>
                </div>
                <div class="authors">
                    Tuhin Chakrabarty, Arkady Saakyan, Olivia Winn, Artemis Panagopoulou, <strong>Yue Yang</strong>, Marianna Apidianaki, Smaranda Muresan
                </div>
                <div class="venue">
                   <i>Findings of the Association for Computational Linguistics (<b>ACL</b>)</i>, 2023
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/pdf/2305.14724">arxiv</a> /
                  <a href="https://github.com/tuhinjubcse/VisualMetaphors">code</a>
                </div>
                <div align="justify">
                  TL;DR: We generate visual metaphors from linguistic metaphors using a collaboration between LLMs and Diffusion Models. We use GPT-3 with Chain-of-Thought prompting to generate text that represents a visual elaboration of the linguistic metaphor, which is then used as input to the diffusion-based text-to-image models to create 6,476 visual metaphors.
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/labo.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/labo.pdf"><b>Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification</b></a>
                </div>
                <div class="authors">
                    <strong>Yue Yang</strong>, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, Mark Yatskar
                </div>
                <div class="venue">
                   <i>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</i>, 2023
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2211.11158">arxiv</a> /
                  <a href="https://github.com/YueYANG1996/LaBo">code</a>
                </div>
                <div align="justify">
                  TL;DR: Concept Bottleneck Models are interpretable models that factor in human-readable concepts to explain model decisions. However, CBMs often under-perform their black box counterparts and require manual specification of concepts. Our method, LaBo, leverages large language models (GPT-3) to automatically construct bottlenecks for any image classification tasks.
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%" style="text-align:center;">
                <img src="images/crepe.png" width="80%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/crepe.pdf"><b>Causal Reasoning About Entities and Events in Procedural Texts</b></a>
                </div>
                <div class="authors">
                    Li Zhang, Hainiu Xu, <strong>Yue Yang</strong>, Shuyan Zhou, Weiqiu You, Manni Arora, Chris Callison-Burch
                </div>
                <div class="venue">
                   <i>Findings of the European Chapter of the ACL (<b>EACL</b>)</i>, 2023
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2301.10896">arxiv</a> /
                  <a href="https://github.com/zharry29/causal_reasoning_of_entities_and_events">code</a>
                </div>
                <div align="justify">
                  TL;DR: We introduces CREPE, a benchmark for causal reasoning about event plausibility based on entity states, it shows that current large language models including GPT-3 perform poorly on this task. To improve the performance, we inject the causal relations between entities and events through structured representations such as programming languages which results in a significant increase in performance.
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/zlavi.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/Z-LaVI.pdf"><b>Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination</b></a>
                </div>
                <div class="authors">
                    <strong>Yue Yang</strong>, Wenlin Yao, Hongming Zhang, Xiaoyang Wang, Dong Yu, Jianshu Chen
                </div>
                <div class="venue">
                   <i>Conference on Empirical Methods in Natural Language Processing (<b>EMNLP</b>)</i>, 2022
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2210.12261">arxiv</a> /
                  <a href="https://github.com/YueYANG1996/Z-LaVI">code</a>
                </div>
                <div align="justify">
                  TL;DR: We develop a novel approach to endow language models with visual imagination capabilities. We leverage two complementary types of "imaginations": (I) recalling existing images through retrieval and (ii) synthesizing nonexistent images via text-to-image generation. Jointly exploiting the language inputs and the imagination, a pretrained vision-language model (e.g., CLIP) eventually composes a zero-shot solution to the original language tasks.
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/property.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/property.pdf"><b>Visualizing the Obvious: A Concreteness-based Ensemble Model for Noun Property Prediction</b></a>
                </div>
                <div class="authors">
                    <strong>Yue Yang</strong>*, Artemis Panagopoulou*, Marianna Apidianaki, Mark Yatskar, Chris Callison-Burch (*equal contribution)
                </div>
                <div class="venue">
                   <i>Findings of the Empirical Methods in Natural Language Processing (<b>EMNLP</b>)</i>, 2022
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2210.12905">arxiv</a> /
                  <a href="https://github.com/artemisp/semantic-norms">code</a>
                </div>
                <div align="justify">
                  TL;DR: We propose to extract properties of nouns from images, which can then be used to complement information from language models to mitigate the reporting bias problem. Results show that the proposed combination of text and images greatly improves noun property prediction compared to powerful language models. 
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/h.gif" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/hierarchy.pdf"><b>Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data</b></a>
                </div>
                <div class="authors">
                    Shuyan Zhou, Harry Li Zhang, <strong>Yue Yang</strong>, Veronica Qing Lyu, Pengcheng Yin, Chris Callison-Burch, Graham Neubig
                </div>
                <div class="venue">
                   <i>Annual Meeting of the Association for Computational Linguistics (<b>ACL</b>)</i>, 2022
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2203.07264">arxiv</a> /
                  <a href="https://github.com/shuyanzhou/wikihow_hierarchy">github</a> /
                  <!-- <a href="bib/yang2021induce.bib">bibtex</a> -->
                  <a href="https://wikihow-hierarchy.github.io/?task_id=104796">website</a>
                </div>
                <div align="justify">
                  TL;DR: Procedures are inherently hierarchical. To "host a party", one may need to "clean the house", which in turn may require "putting away the clothes". We develop an efficient method that links steps (e.g. "clean the house") in an article to other articles with similar intents (e.g. "how to deep lean your house"), which proceeds recursively to form the KB. 
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/IER.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/IER.pdf"><b>Induce, Edit, Retrieve: Language Grounded Multimodal Schema for Instructional Video Retrieval</b></a>
                </div>
                <div class="authors">
                    <strong>Yue Yang</strong>, Joongwon Kim, Artemis Panagopoulou, Mark Yatskar and Chris Callison-Burch
                </div>
                <div class="venue">
                  <i>CVPR 2022 @ ODRUM</i>, 2022, spotlight talk
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2111.09276">arxiv</a> /
                  <a href="bib/yang2021induce.bib">bibtex</a>
                </div>
                <div align="justify">
                  TL;DR: This work proposes a novel system that induces schemata from web videos and generalizes them to capture unseen tasks with the goal of improving video retrieval performance, and demonstrates that the schemata induced by the system are better than those generated by other models.
                </div>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img src="images/VGSI.png" width="100%">
              </td>
              <td width="70%" valign="middle">
                <div class="title">
                  <a href="papers/VGSI.pdf"><b>Visual Goal-Step Inference using wikiHow</b></a>
                </div>
                <div class="authors">
                    <strong>Yue Yang</strong>, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar and Chris Callison-Burch
                </div>
                <div class="venue">
                  <i>Conference on Empirical Methods in Natural Language Processing (<b>EMNLP</b>)</i>, 2021
                </div>
                <div class="tags">
                  <a href="https://arxiv.org/abs/2104.05845">arxiv</a> / 
                  <a href="bib/yang2021visual.bib">bibtex</a> /
                  <a href="https://www.youtube.com/watch?v=V3Y_56ykG54&t=11s">talk</a> /
                  <a href="https://github.com/YueYANG1996/wikiHow-VGSI">github</a>
                </div>
                <div align="justify">
                  TL;DR: This work proposes the Visual Goal-Step Inference (VGSI) task where a model is given a textual goal and must choose a plausible step towards that goal from among four candidate images. We construct a VGSI dataset from wikiHow and show that SOTA multimodal models struggle on it. 
                </div>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Education</heading>
              <table>
                <tr>
                  <td width="85%">
                    <b>University of Pennsylvania</b>, Philadelphia, PA, USA<br>
                    <li> Ph.D. in Computer and Information Science (2020 - present)</li>
                    <li> M.S. in Robotics (2018 - 2020)</li>
                  </td>
                  <td width="15%">
                    <img src="images/penn-logo.png" width="100%" style="display: block; margin-left: auto; margin-right: auto;">
                  </td>
                </tr>
                <tr>
                  <td width="85%">
                    <b>Zhejiang University</b>, Hangzhou, China<br>
                    <li> B.E. in Mechanical Engineering (2014 - 2018)</li>
                  </td>
                  <td width="15%">
                    <img src="images/zju-logo.png" width="100%" style="display: flex; justify-content: center;">
                  </td>
                </tr>
              </table>
            </td>
          </tr>
          </table>

          <!-- Experience -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Experiences</heading>
              <table>
                <tr>
                  <td width="85%">
                    <a href="https://prior.allenai.org"><b>Allen Institute for AI</b></a>, Seattle, WA, USA<br>
                    <i>Research Intern</i> (May. 2023 to Sept. 2023, May. 2024 to Sept. 2024)<br>
                    <b style="color: #ac530f">Outstanding Intern of the Year Award (2023)</b> <br>
                  </td>
                  <td width="15%">
                    <img src="images/ai2.png" height=50px style="display: block; margin-left: auto; margin-right: auto;">
                  </td>
                </tr>
                <br>
                <tr>
                  <td width="85%">
                    <a href="https://ai.tencent.com/ailab/nlp/en/index.html"><b>Tencent AI Lab</b></a>, Seattle, WA, USA<br>
                    <i>Research Scientist Intern</i> (May. 2022 to Sept. 2022)
                  </td>
                  <td width="15%">
                    <img src="images/tencent-logo.webp" height=50px style="display: block; margin-left: auto; margin-right: auto;">
                  </td>
                </tr>
              </table>
            </td>
          </tr>
          </table>

          <!-- Teaching -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Teaching</heading><br><br>
              <strong>Head Teaching Assistant</strong>, <a href="https://artificial-intelligence-class.org/">CIS-521 Artificial Intelligence</a>, University of Pennsylvania<br>
              Fall2019; Fall 2020; Summer 2021; Fall 2021; Spring 2022 <br> <br>
              <strong>Teaching Assistant</strong>, <a href="http://markyatskar.com/cis530_sp2021/">CIS-530 Computational Linguistics</a>, University of Pennsylvania<br>
              Spring 2021 <br>
            </td>
          </tr>
          </table>

          <!-- Academic Service -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Academic Service</heading><br><br>
                <strong>Reviewer</strong>: <br>
                Computer Vision: CVPR, ECCV, SIGGRAPH Asia. <br>
                Natural Language Processing: ACL, EMNLP, NAACL, EACL, COLM. <br>
                Machine Learning: NeurIPS, TMLR, ICLR. <br>
              </td>
            </tr>
            </table>

          <!-- Talks -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Talks</heading>
              <table>
                <tr>
                  <td width="100%">
                    <b>WPE-II Presentation</b>, University of Pennsylvania, Philadelphia, PA, USA<br>
                    <i>Language Guided Concept Bottlenecks for Interpretable and Robust Image Classification</i>, April 29, 2024. <a href="slides/bottleneck_wpe.pdf">slides</a> 
                  </td>
                </tr>
                <tr>
                  <td width="100%">
                    <a href="https://nlp.cis.upenn.edu/clunch.html"><b>CLUNCH</b></a>, University of Pennsylvania, Philadelphia, PA, USA<br>
                    <i>Investigate Procedural Events in a Multimodal Fashion</i>, November 22, 2021. <a href="slides/CLUNCH_talk.pdf">slides</a> 
                  </td>
                </tr>
              </table>
            </td>
          </tr>
          </table>

          <!-- Acknowledgements -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
              <td><br>
              <p align="right">
              <font size="2">
              Website source from <a href="https://jonbarron.info">Jon Barron</a>.
          </tr>
          </table>
        </td>
      </tr>
    </table>
  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=QLmiHQflxIRMm2ULgewSTH1RI_kP-_LEhBP5p6IrxxE&cl=ffffff&w=300"></script>
  </body>
<script>'undefined'=== typeof _trfq || (window._trfq = []);'undefined'=== typeof _trfd && (window._trfd=[]),_trfd.push({'tccl.baseHost':'secureserver.net'}),_trfd.push({'ap':'cpsh-oh'},{'server':'p3plzcpnl472835'},{'id':'7914943'}) // Monitoring performance to make your website faster. If you want to opt-out, please contact web hosting support.</script><script src='https://img1.wsimg.com/tcc/tcc_l.combined.1.0.6.min.js'></script></html>